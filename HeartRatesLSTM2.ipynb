{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9cc62b6-556d-41d8-bdea-8c9877cd3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f108b4a3-f696-4653-8093-435afaa9fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def build_sequence_dataset(df, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(df) - seq_length):\n",
    "        _x = df.iloc[i:i + seq_length].values\n",
    "        _y = df.iloc[i + seq_length]['Value']\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0, c0 = self.init_hidden(batch_size, x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return h0, c0\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    actuals= []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target.view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            actuals.extend(target.squeeze(1).tolist())\n",
    "            predictions.extend(output.squeeze(1).tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss, actuals, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a2ee919-9895-4cb5-9a93-9ef922ddc6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|██▌                      | 10/100 [01:11<10:42,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 10: train loss(mse) = 0.1373 test loss(mse) = 0.1653\n",
      "학습 완료 : 총 10 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|█████                    | 20/100 [02:21<09:16,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 20: train loss(mse) = 0.1367 test loss(mse) = 0.1652\n",
      "학습 완료 : 총 20 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|███████▌                 | 30/100 [03:31<08:13,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 30: train loss(mse) = 0.1363 test loss(mse) = 0.1642\n",
      "학습 완료 : 총 30 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|██████████               | 40/100 [04:42<07:08,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 40: train loss(mse) = 0.1361 test loss(mse) = 0.1632\n",
      "학습 완료 : 총 40 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  50%|████████████▌            | 50/100 [05:53<05:56,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 50: train loss(mse) = 0.1359 test loss(mse) = 0.1639\n",
      "학습 완료 : 총 50 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  60%|███████████████          | 60/100 [07:06<04:49,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 60: train loss(mse) = 0.1357 test loss(mse) = 0.1631\n",
      "학습 완료 : 총 60 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  70%|█████████████████▌       | 70/100 [08:17<03:34,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 70: train loss(mse) = 0.1355 test loss(mse) = 0.1627\n",
      "학습 완료 : 총 70 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  80%|████████████████████     | 80/100 [09:29<02:23,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 80: train loss(mse) = 0.1354 test loss(mse) = 0.1628\n",
      "학습 완료 : 총 80 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  90%|██████████████████████▌  | 90/100 [10:42<01:14,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 90: train loss(mse) = 0.1353 test loss(mse) = 0.1625\n",
      "학습 완료 : 총 90 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|████████████████████████| 100/100 [11:56<00:00,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 100: train loss(mse) = 0.1352 test loss(mse) = 0.1629\n",
      "학습 완료 : 총 100 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_ppg = pd.read_csv('/home/youngwoo/export_cda.csv')\n",
    "df_ppg = df_ppg[df_ppg['MeasureName'] == 'Heart rate']['Value'].to_frame()\n",
    "\n",
    "seed_value = 42\n",
    "set_seed(seed_value)\n",
    "\n",
    "num_output = 1\n",
    "num_hidden = 10\n",
    "num_features = 1\n",
    "\n",
    "seq_length = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ppg_scaled_arr = scaler.fit_transform(df_ppg)\n",
    "df_ppg_scaled = pd.DataFrame(ppg_scaled_arr, columns=['Value']) \n",
    "\n",
    "sequence_dataX, sequence_dataY = build_sequence_dataset(df_ppg_scaled, seq_length)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "    sequence_dataX, sequence_dataY, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_Y_tensor = torch.tensor(train_Y.reshape(-1, 1), dtype=torch.float32)\n",
    "test_X_tensor = torch.tensor(test_X, dtype=torch.float32)\n",
    "test_Y_tensor = torch.tensor(test_Y.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model_lstm = LSTMModel(input_size=num_features, hidden_size=num_hidden, num_layers=1, output_size=num_output).to(device)\n",
    "\n",
    "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=learning_rate)\n",
    "criterion_lstm = nn.MSELoss()\n",
    "\n",
    "train_loss_lst = []\n",
    "test_loss_lst = []\n",
    "\n",
    "max_epochs= 100\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "for epoch in tqdm(range(max_epochs), desc=\"Training Epochs\"):  # tqdm 사용\n",
    "    train_loss = train(model_lstm, train_loader, optimizer_lstm, criterion_lstm)\n",
    "    test_loss, actuals, predictions = validate_model(model_lstm, test_loader, criterion_lstm)\n",
    "    train_loss_lst.append(train_loss)\n",
    "    test_loss_lst.append(test_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"\\nepoch {epoch+1}: train loss(mse) = {train_loss:.4f} test loss(mse) = {test_loss:.4f}\")\n",
    "\n",
    "        print(f\"학습 완료 : 총 {epoch+1} epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a17e0-d6a1-4183-9a88-32c3340afb08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
